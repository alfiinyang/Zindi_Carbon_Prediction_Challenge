# -*- coding: utf-8 -*-
"""EY_Carbon_Prediction_StarterNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oxs5oeZXniOBNbd50iF9GDg4bKUoE5y9

<h2><center> Welcome to the Ernst & Young Carbon Prediction Hackathon</h2></center>
<figure>
<!-- <center><img src ="https://drive.google.com/uc?export=view&id=1hSOAfRhJ_jo-MZAjq81VYJu5bZNL7EjD" width = "800" height = '500'/> -->

*About the problem*
> The ability to accurately monitor carbon emissions is a critical step in the fight against climate change. Precise carbon readings allow researchers and governments to understand the sources and patterns of carbon mass output. While Europe and North America have extensive systems in place to monitor carbon emissions on the ground, there are few available in Africa.

*Objective of this challenge*
> The objective of this challenge is to create machine learning or a deep learning model using open-source CO2 emissions data (from Sentinel-5P satellite observations) to predict carbon emissions.

These solutions will enable EY, governments, and other actors to estimate carbon emission levels across Africa, even in places where on-the-ground monitoring is not possible.
"""

from google.colab import drive
drive.mount('/content/drive')

# runtime info
import time
from datetime import timedelta
start_time = time.monotonic()
metadata = {}

"""## Table of contents:

1. [Installing and importing libraries](#Libraries)
2. [Loading data](#Data)
3. [Statistical summaries](#Statistics)
4. [Outliers](#Outliers)
5. [Geo Visualisation - EDA](#Geo)
5. [Missing values and duplicates](#Missing)
6. [Date features EDA](#Dates)
7. [Correlations - EDA](#Correlations)
9. [Timeseries visualization - EDA](#Timeseries)
10. [Feature engineering](#Engineering)
11. [Modelling](#Modelling)
12. [Making predictions of the test set and creating a submission file](#Predictions)

<a name="Libraries"></a>
## 1. Installing and importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Install relevant libraries
# !pip install geopandas folium

# Commented out IPython magic to ensure Python compatibility.
# Import libraries
import pandas as pd
import numpy as np
import random
import os
from tqdm.notebook import tqdm

import geopandas as gpd
from shapely.geometry import Point
import folium

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
pd.options.display.float_format = '{:.5f}'.format
pd.options.display.max_rows = None

# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

# Set seed for reproducability
SEED = 5000
random.seed(SEED)
np.random.seed(SEED)

"""<a name="Data"></a>
## 2. Loading and previewing data
"""

DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/EY_Carbon_Prediction_Files'
# Load files
train = pd.read_csv(os.path.join(DATA_PATH, 'Train.csv'))
test = pd.read_csv(os.path.join(DATA_PATH, 'Test.csv'))
samplesubmission = pd.read_csv(os.path.join(DATA_PATH, 'SampleSubmission.csv'))

# Preview train dataset
train.head()

# Preview test dataset
test.head()

# Preview sample submission file
samplesubmission.head()

# Check size and shape of datasets
train.shape, test.shape, samplesubmission.shape
data_shape_before = [train.shape, test.shape]

# Train to test sets ratio
(test.shape[0]) / (train.shape[0] + test.shape[0])

"""<a name="Statistics"></a>
## 3. Statistical summaries
"""

# Train statistical summary
train.describe(include = 'all')

"""From the above statistical summary, we can deduce some of the following insights:
 - The train data provided ranges from year *2019 to 2022*
 - Minimum recorded CO2 emissions is *0.04806* and a maximum of *1181701.2*
 - Week of the year starts from 1 to 52
 - The latitude and longitudes ranges show that the regions are mostly within South Africa 
"""

# Target variable distribution

# Log transformation of target variable
train['emission'] = np.log1p(train['emission'])

sns.set_style('darkgrid')
plt.figure(figsize = (13, 7))
sns.histplot(train.emission, kde = True, bins = 15)
plt.title('Target variable distribution', y = 1.02, fontsize = 15)
display(plt.show(), train.emission.skew())

"""The target variable is skewed to the right with a a degree of ~17.

Some of the techniques used to handle skewness include:
- Log transform
- Box-cox transform
- Square root transform
- *etc*

<a name="Outliers"></a>
## 4. Outliers
"""

# Removing the outlier
# train = train[train['emission'] < train['emission'].quantile(0.995)]

# Plotting boxplot for the CO2 emissions
sns.set_style('darkgrid')
plt.figure(figsize = (13, 7))
sns.boxplot(train.emission)
plt.title('Boxplot showing CO2 emission outliers', y = 1.02, fontsize = 15)  
plt.show()

"""Outliers are those data points which differ significantly from other observations present in given dataset.

Suggestions on how to handle outliers:
 - Transforming the outliers by scaling - log transformation, box-cox transformation ...
 - Dropping outliers
 - Imputation by replacing outliers with mean, median ...

<a name="Geo"></a>
## 5. Geo Visualisation - EDA
"""

# Combine train and test for easy visualisation
train_coords = train.drop_duplicates(subset = ['latitude', 'longitude'])
test_coords = test.drop_duplicates(subset = ['latitude', 'longitude'])
train_coords['set_type'], test_coords['set_type'] = 'train', 'test'

all_data = pd.concat([train_coords, test_coords], ignore_index = True)
# Create point geometries

geometry = gpd.points_from_xy(all_data.longitude, all_data.latitude)
geo_df = gpd.GeoDataFrame(
    all_data[["latitude", "longitude", "set_type"]], geometry=geometry
)

# Preview the geopandas df
geo_df.head()

# Create a canvas to plot your map on
all_data_map = folium.Map(prefer_canvas=True)

# Create a geometry list from the GeoDataFrame
geo_df_list = [[point.xy[1][0], point.xy[0][0]] for point in geo_df.geometry]

# Iterate through list and add a marker for each volcano, color-coded by its type.
i = 0
for coordinates in geo_df_list:
    # assign a color marker for the type set
    if geo_df.set_type[i] == "train":
        type_color = "green"
    elif geo_df.set_type[i] == "test":
        type_color = "orange"

    # Place the markers 
    all_data_map.add_child(
        folium.CircleMarker(
            location=coordinates,
            radius = 1,
            weight = 4,
            zoom =10,
            popup= 
            "Set: " + str(geo_df.set_type[i]) + "<br>"
            "Coordinates: " + str([round(x, 2) for x in geo_df_list[i]]),
            color =  type_color),
        )
    i = i + 1
all_data_map.fit_bounds(all_data_map.get_bounds())
all_data_map

"""<a name="Missing"></a>
## 6. Missing values and duplicates
"""

# Check for missing values
train.isnull().sum().any(), test.isnull().sum().any()

# Check for duplicates
train.duplicated().any(), test.duplicated().any()

"""#### 6.1. Missing values in `train` dataset"""

# Plot missing values in train set
ax = train.isna().sum().sort_values(ascending = False)[:15][::-1].plot(kind = 'barh', figsize = (9, 10))
plt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})
for p in ax.patches:
    percentage ='{:,.0f}%'.format((p.get_width()/train.shape[0])*100)
    width, height =p.get_width(),p.get_height()
    x=p.get_x()+width+0.02
    y=p.get_y()+height/2
    ax.annotate(percentage,(x,y))

"""#### 6.2. Missing values in `test` dataset"""

# Plot missing values in test set
ax = test.isna().sum().sort_values(ascending = False)[:15][::-1].plot(kind = 'barh', figsize = (9, 10))
plt.title('Percentage of Missing Values Per Column in test Set', fontdict={'size':15})
for p in ax.patches:
    percentage ='{:,.0f}%'.format((p.get_width()/test.shape[0])*100)
    width, height =p.get_width(),p.get_height()
    x=p.get_x()+width+0.02
    y=p.get_y()+height/2
    ax.annotate(percentage,(x,y))

"""Suggestions on how to handle missing values:
 - Fill in missing values with mode, mean, median..
 - Drop Missing datapoints with missing values
 - Fill in with a large number e.g -999999

### By Dropping

function `df_dropna()` for dropping columns with missing values
"""

def df_dropna(df):
  # droppng missing variables in df dataset

  # Calculate the percentage of missing values in each column of the dataset
  df_missing_percentages = df.isnull().sum() / df.shape[0] * 100

  # Select columns with missing values >= 70% of total observations
  df_high_missing_cols = df_missing_percentages[df_missing_percentages >= 70].index.tolist()

  # Print the list of high missing value columns and their count
  print('Before Drop:')
  print(f"Columns with missing values above 70%: {df_high_missing_cols}")
  print(f"Number of columns with missing values above 70%: {len(df_high_missing_cols)}\n")

  # drop all columns with with over 70% missing values
  if df.isnull().sum().sum()/len(df.index)*100 >= 70:
    df = df.dropna(thresh=len(df)*0.7, axis=1)

  # After drop
  df_missing_percentages = df.isnull().sum() / df.shape[0] * 100
  df_high_missing_cols = df_missing_percentages[df_missing_percentages >= 70].index.tolist()

  print('After Drop:')
  print(f"Columns with missing values above 70%: {df_high_missing_cols}")
  print(f"Number of columns with missing values above 70%: {len(df_high_missing_cols)}")

"""### By filling

function `median_clean()` for filling missing values with median
"""

data_shape_mid = [train.shape, test.shape]
data_shape_after = list

def median_clean(train,test):
  # # fill missing values with mean of each column
  train = train.fillna(train.median())
  
  # Train After fill
  train_missing_percentages = train.isnull().sum() / train.shape[0] * 100
  train_high_missing_cols = train_missing_percentages[train_missing_percentages >= 70].index.tolist()

  print('Train After fill:')
  print(f"Columns with missing values above 70%: {train_high_missing_cols}")
  print(f"Number of columns with missing values above 70%: {len(train_high_missing_cols)}\n")

  test = test.fillna(test.median())

  # Test After fill
  test_missing_percentages = test.isnull().sum() / test.shape[0] * 100
  test_high_missing_cols = test_missing_percentages[test_missing_percentages >= 70].index.tolist()

  print('Test After fill:')
  print(f"Columns with missing values above 70%: {test_high_missing_cols}")
  print(f"Number of columns with missing values above 70%: {len(test_high_missing_cols)}")

  #data_shape_after = [train.shape, test.shape]

"""### execute one cleaning option"""

# execute one data clean
# median_clean(train,test,metadata); metadata = {'clean_action':'fill_na', 'value':'median'}

df_dropna(train), df_dropna(test); metadata = {'clean_action':'drop_na', 'value':'-'}

"""<a name="Dates"></a>
## 7. Date features EDA
"""

# Year countplot
plt.figure(figsize = (14, 7))
sns.countplot(x = 'year', data = train)
plt.title('Year count plot')
plt.show()

"""- The number of observations of CO2 emissions are relatively the same across the years
- Year 2022 has fewer number of observations
"""

# Week countplot
plt.figure(figsize = (14, 7))
sns.countplot(x = 'week_no', data = train)
plt.title('Week count plot')
plt.show()

"""- The number of observations of CO2 emissions are relatively the same across the weeks
- Weeks 49, 50 and 51  have fewer number of observations when compared to the other weeks
"""

train.drop_duplicates(subset = ['year', 'week_no']).groupby(['year'])[['week_no']].count()

"""- The year 2022 has only 49 weeks available in the data

<a name="Correlations"></a>
## 8. Correlations - EDA
"""

# Top 20 correlated features to the target
top20_corrs = abs(train.corr()['emission']).sort_values(ascending = False).head(20)
top20_corrs

# Quantify correlations between features
corr = train[list(top20_corrs.index)].corr()
plt.figure(figsize = (13, 8))
sns.heatmap(corr, cmap='RdYlGn', annot = True, center = 0)
plt.title('Correlogram', fontsize = 15, color = 'darkgreen')
plt.show()

"""<a name="Timeseries"></a>
## 9. Timeseries visualization - EDA
"""

# Sample a unique location and visualize its emissions across the years
train.latitude, train.longitude = round(train.latitude, 2), round(train.longitude, 2)
sample_loc = train[(train.latitude == -23.73) & (train.longitude == 28.77)]

# Plot a line plot
sns.set_style('darkgrid')
fig, axes = plt.subplots(nrows = 4, ncols = 1, figsize = (13, 10))
fig.suptitle('Co2 emissions for location lat -23.75 lon 28.75', y=1.02, fontsize = 15)

for ax, data, year, color, in zip(axes.flatten(), sample_loc, sample_loc.year.unique(), ['#882255','#332288', '#999933' , 'orangered']):
  df = sample_loc[sample_loc.year == year]
  # sns.lineplot(df.week_no, df.emission, ax = ax, label = year, color = color)
  sns.lineplot(x='week_no', y='emission', data=df, ax=ax, label=year, color=color)

plt.legend()
plt.tight_layout()

"""<a name="Engineering"></a>
## 10. Feature engineering








"""

# Examples of feature engineering - Aggregations, cumulative differences, moving averages ...
# Lets explore the rolling mean
# First we create a unique location from lat lon
train['location'] = [str(x) + '_' + str(y) for x, y in zip(train.latitude, train.longitude)]

# Filter based on one location
example_loc = train[train.location == '-23.73_28.77']

# Calculate rolling mean for SulphurDioxide_SO2_column_number_density_amf with a window of 2 weeks
rolling_mean = example_loc['SulphurDioxide_SO2_column_number_density_amf'].rolling(window = 2).mean()

# Visualise rolling mean
plt.figure(figsize = (15, 7))
rolling_mean.plot()
plt.title('Rolling mean with a window of 2 weeks for SulphurDioxide_SO2_column_number_density_amf', y = 1.02, fontsize = 15)
plt.xlabel('week', y = 1.05, fontsize = 13)
plt.ylabel('SulphurDioxide_SO2_column_number_density_amf', x = 1.05, fontsize = 13)
plt.show()

""" - There is a cyclic pattern from the above graph, which clearly shows that there is a pattern - This will be helpful to our model
 - With more research and domain knowledge generate useful features that can improve your model performance

 Other examples of feature engineering:
  - Creating cluster regions
  - Interactions between different pollutatnts - ratios, additions,subtractions...
  - Time series features
"""

# Generate the above feature - rolling mean for all locations for both the train and test

# Feature engineering train
train_roll_mean = train.sort_values(by = ['location', 'year', 'week_no']).groupby(['location'])[train.columns[5:].tolist()].rolling(window = 2).mean().reset_index()
train_roll_mean.drop(['level_1', 'emission', 'location'], axis = 1, inplace = True)
train_roll_mean.columns = [col + '_roll_mean' for col in train_roll_mean.columns]

# Feature engineering test
test.latitude, test.longitude = round(test.latitude, 2), round(test.longitude, 2)
test['location'] = [str(x) + '_' + str(y) for x, y in zip(test.latitude, test.longitude)]
test_roll_mean = test.sort_values(by = ['location', 'year', 'week_no']).groupby(['location'])[test.columns[5:].tolist()].rolling(window = 2).mean().reset_index()
test_roll_mean.drop(['level_1', 'location'], axis = 1, inplace = True)
test_roll_mean.columns =  [col + '_roll_mean' for col in test_roll_mean.columns]
test_roll_mean.head()

# Merge engineered features with train and test set

#Train
train_eng = train.sort_values(by = ['location', 'year', 'week_no'], ignore_index = True).merge(train_roll_mean, how = 'left',
                                                                                               left_index=True, right_index=True)

# Test
test_eng = test.sort_values(by = ['location', 'year', 'week_no'], ignore_index = True).merge(test_roll_mean, how = 'left',
                                                                                               left_index=True, right_index=True)

# Preview engineered test set
test_eng.head()

"""<a name="Modelling"></a>
## 11.  Modelling
"""

# Selecting the independent variables and the target variable

X = train_eng.drop(['ID_LAT_LON_YEAR_WEEK', 'location', 'emission'], axis = 1).fillna(0)
y = train_eng.emission

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)

# Instantiating the model
clf = RandomForestRegressor(random_state = SEED, n_jobs=-1)
clf.fit(X_train, y_train)

# Making predictions
y_pred = clf.predict(X_test)

# Measuring the accuracy of the model
RMSE_Score = mean_squared_error(y_test, y_pred, squared=False)
print(f'RMSE Score: {RMSE_Score}') # 23432.342352754695

X_test.head()

# Analyse predictions
pred_errors = X_test.copy()
pred_errors['emission'] = y_test
pred_errors['prediction'] = y_pred
pred_errors['error'] = abs(pred_errors.prediction - pred_errors.emission)
pred_errors = pred_errors[['latitude',	'longitude',	'year',	'week_no', 'emission', 'prediction', 'error']]
pred_errors.sort_values(by = 'error', ascending = False, inplace = True)
pred_errors.head()

pred_errors.tail()

train.emission.describe()

# Feature importance
impo_df = pd.DataFrame({'feature': X.columns, 'importance': clf.feature_importances_}).set_index('feature').sort_values(by = 'importance', ascending = False)
impo_df = impo_df[:12].sort_values(by = 'importance', ascending = True)
impo_df.plot(kind = 'barh', figsize = (10, 10))
plt.legend(loc = 'center right')
plt.title('Bar chart showing feature importance', fontsize = 14)
plt.xlabel('Features', fontsize = 12)
plt.show()

"""<a name="Predictions"></a>
## 12. Making predictions of the test set and creating a submission file
"""

# Make prediction on the test set
test_df = test_eng.drop(['ID_LAT_LON_YEAR_WEEK', 'location'], axis = 1).fillna(0)
predictions = clf.predict(test_df)

# # Create a submission file
sub_file = pd.DataFrame({'ID_LAT_LON_YEAR_WEEK': test_eng.ID_LAT_LON_YEAR_WEEK, 'emission': predictions})
sub_file.head()

# model_info = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/EY_Carbon_Prediction_Files/models_metadata.csv')

"""####12.1. create iteration metadata"""

model_info = pd.read_csv(os.path.join(DATA_PATH,'models_metadata.csv'))

from datetime import date
import time

# date/time info
date_now = date.today().strftime("%d-%m-%Y")
time_now = time.strftime("%H_%M_%S")
iterr = date_now + '-@-' + time_now

# cleaning info

#subfile_path = '/content/drive/MyDrive/Colab Notebooks/EY_Carbon_Prediction_Files/'+iterr+'_Submission_file.csv'
#prederrors_path = '/content/drive/MyDrive/Colab Notebooks/EY_Carbon_Prediction_Files/'+iterr+'_pred_err.csv'

metadata['RMSE_Score'] = RMSE_Score
metadata['iter_name'] = 'submission_'+iterr
metadata['pred_err_file'] = os.path.join(DATA_PATH, 'pred_err_'+iterr+'.csv')

# Create file
if sub_file.shape == samplesubmission.shape:
  print(f'good to go! \nsub_file & samplesubmission shape match {sub_file.shape, samplesubmission.shape}')
  sub_file.to_csv(os.path.join(DATA_PATH, 'submission_'+iterr+'.csv'), index = False) # Download subfile and submit to zindi for scoring
  pred_errors.to_csv(os.path.join(DATA_PATH, 'pred_err_'+iterr+'.csv'), index = False)
else:
  print('ERROR: something\'s missing')

# runtime info
end_time = time.monotonic()
runtime = timedelta(seconds=end_time - start_time)
print(f'runtime: {runtime}\n') # move to end of code

metadata['runtime'] = runtime
metadatum = pd.DataFrame(metadata, index=range(len(metadata)))

model_info=pd.concat([model_info,metadatum], ignore_index=True )
model_info.to_csv(os.path.join(DATA_PATH,'models_metadata.csv'), index=False)

"""## ALL THE BEST AND HAVE FUN &#x1F60E;"""
